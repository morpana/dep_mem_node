{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the memory node networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following document outlines generating the weights for the networks used in the memory node of the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-amble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for manipulating DEP generated and stored C matrices from file\n",
    "# Allows for matrix to be loaded from file created by dep_gui\n",
    "# A reduced version of these matrices are used as most muscles are inactive obtained via reduced_matrix()\n",
    "class matrix_expansion:\n",
    "    \n",
    "    def __init__(self,active_motors):\n",
    "        self.active_motors = active_motors\n",
    "        active_sensors = np.array(active_motors*2)\n",
    "        active_sensors[len(active_motors):] += 14\n",
    "        self.active_sensors = active_sensors\n",
    "        self.shape = ()\n",
    "        \n",
    "    def load_from_file(self,filename):\n",
    "        f = open(filename,\"r\")\n",
    "        matrix = f.read()\n",
    "        f.close()\n",
    "        matrix = re.split(\",NEW_ROW,\",matrix)\n",
    "        matrix.pop()\n",
    "        matrix = np.array([np.array(re.split(\",\", row)).astype(np.float) for row in matrix])\n",
    "        self.shape = matrix.shape\n",
    "        return matrix\n",
    "        \n",
    "    def reduced_matrix(self,matrix):\n",
    "        matrix = matrix[:,self.active_sensors][self.active_motors]\n",
    "        return matrix\n",
    "    \n",
    "    def expanded_matrix(self,reduced_matrix):\n",
    "        matrix = np.zeros(self.shape)\n",
    "        flat = reduced_matrix.flatten()\n",
    "        matrix = np.zeros((14,28))\n",
    "        k = 0\n",
    "        for i in active_motors:\n",
    "            for j in active_sensors:\n",
    "                matrix[i,j] = flat[k]\n",
    "                k += 1\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "\n",
    "# HTM SDR Scalar Encoder\n",
    "# Input: Scalar\n",
    "# Parameters: n - number of units, w - bits used to represent signal (width), b - buckets (i.e. resolution), \n",
    "#             min - minimum value of input (inclusive), max - maximum input value (inclusive)\n",
    "class scalar_sdr:\n",
    "    \n",
    "    def __init__(self, b, w, min_, max_, shape=(1,1), neg=True):\n",
    "        if type(b) != int or type(w) != int or type(min_) != float or type(max_) != float:\n",
    "            raise TypeError(\"b - buckets must be int, w - width must be int, min_ must be float and max_ must be float\")\n",
    "        self.b = b # must be int\n",
    "        self.w = w # must be int\n",
    "        self.min = min_ # must be float\n",
    "        self.max = max_ # must be float\n",
    "        self.n = b+w-1 # number of units for encoding\n",
    "        self.ndarray_shape = shape\n",
    "        self.nodes = self.n*reduce(lambda x, y: x*y, self.ndarray_shape)\n",
    "        self.neg = neg\n",
    "        \n",
    "    def encode(self,input_):\n",
    "        if input_ > self.max or input_ < self.min:\n",
    "            raise ValueError(\"Input outside encoder range!\")\n",
    "        if type(input_) != float:\n",
    "            raise TypeError(\"Input must be float!\")\n",
    "        if self.neg:\n",
    "            output = np.zeros(self.n)-1\n",
    "        else:\n",
    "            output = np.zeros(self.n)\n",
    "        index = int((input_-self.min)/(self.max-self.min)*self.b)\n",
    "        output[index:index+self.w] = 1\n",
    "        return output\n",
    "    \n",
    "    def encode_ndarray(self,input_):\n",
    "        if input_.shape != self.ndarray_shape:\n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        output = []\n",
    "        for i in np.nditer(input_, order='K'):\n",
    "            output.append(self.encode(float(i)))\n",
    "        return np.array(output)\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: # or len(np.nonzero(input_+1)[0]) != self.w: <-- Can't have since the network is not guaranteed to produce this by any means!!!\n",
    "            raise TypeError(\"Input does not correspond to encoder encoded data!\")\n",
    "        # output = np.nonzero(input_+1)[0][0]/float(self.b)*(self.max-self.min)+self.min <-- This doesn't work really since bits can randomly fire, taking the average is a more reasonable decoding\n",
    "        median = np.median(np.nonzero(input_+1)[0])            \n",
    "        try:\n",
    "            output = int(median-float(self.w)/2.0)/float(self.b)*(self.max-self.min)+self.min # i.e. figure out center (median more outlier resistant than mean) and subtract width/2\n",
    "        except ValueError:\n",
    "            output = None\n",
    "        return output\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: \n",
    "            raise TypeError(\"Input length does not match encoder length!\")\n",
    "        if len(np.nonzero(input_+1)[0]) == 0:\n",
    "            return np.nan\n",
    "        max_ = 0\n",
    "        output = 0.0\n",
    "        for i in range(self.b):\n",
    "            x = np.zeros(self.n)-1\n",
    "            x[i:i+self.w] = 1\n",
    "            if x.shape != input_.shape:\n",
    "                input_ = input_.reshape(x.shape)\n",
    "            score = np.inner(x,input_)\n",
    "            if score > max_:\n",
    "                max_ = score\n",
    "                output = float(i)/float(self.b)*(self.max-self.min)+self.min\n",
    "        return output\n",
    "            \n",
    "    def decode_ndarray(self,input_):\n",
    "        if input_.shape != (reduce(lambda x, y: x*y, self.ndarray_shape)*self.n,): \n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        input_ = input_.reshape(self.ndarray_shape+(self.n,))\n",
    "        output = []\n",
    "        for i in np.ndindex(self.ndarray_shape):\n",
    "            output.append(self.decode(input_[i]))\n",
    "        output = np.array(output).reshape(self.ndarray_shape)\n",
    "        return output\n",
    "    \n",
    "    def set_ndarray_shape(self,shape):\n",
    "        if type(shape) != tuple:\n",
    "            raise TypeError(\"Must provide tuple of array dimensions!\")\n",
    "        self.ndarray_shape = shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Associative Memory (LAM) behavior weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section generates the weights for the LAM network used in the memory node from four desired behaviors: i) zero--no behavior; ii) front back; iii) front side; and iv) side down. For more information on each of the behaviors you can refer to the thesis or experiment with them for yourself with the GUI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAM Class\n",
    "class LAM:\n",
    "    def __init__(self,shape,weights=None):\n",
    "        self.shape = shape\n",
    "        try:\n",
    "            if weights == None:\n",
    "                self.weights = np.zeros(shape)\n",
    "        except ValueError:\n",
    "            self.weights = weights\n",
    "    \n",
    "    def store(self,input,output):\n",
    "        dW = np.outer(input,output)\n",
    "        self.weights += dW\n",
    "        \n",
    "    def recall(self,input,threshold=0,print_=False):\n",
    "        output = input.dot(self.weights)-threshold\n",
    "        if print_ == True:\n",
    "            print output\n",
    "        output[output > 0] = 1\n",
    "        output[output < 0] = 0\n",
    "        return output\n",
    "\n",
    "    def save_weights(self,filename):\n",
    "        np.save(filename, self.weights)\n",
    "        \n",
    "    def load_weights(self,filename):\n",
    "        weights = np.load(filename)\n",
    "        if weights.shape == self.shape:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions of specified weight array does not match network weight dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain encoder\n",
    "# Buckets = 1000, width = 21, range = 1000\n",
    "brain_encoder = scalar_sdr(1000,21,0.,1000.,neg=False)\n",
    "\n",
    "# We have four behaviors that we want to store in the network\n",
    "names = [\"zero\",\"fb\",\"fs\",\"sd\"]\n",
    "\n",
    "# For each of these behaviors lets create an integer ID\n",
    "# We need the ids to have no overlap and thus they are seperated by the width of the brain_encoder\n",
    "brain_ids = [float(i)*brain_encoder.w for i in range(len(names))]\n",
    "\n",
    "# This is just a dictionary associating each of the labels to the integer ID\n",
    "behaviors = dict(zip(names,brain_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrix data for desired behaviors from matrix files generated by GUI\n",
    "# Note: need to define desired behaviors from set of files in /{HOME_ENVIRONMENT}/dep_matrices\n",
    "# The matrices are located in the \"home\" directory if new behaviors are generated with the GUI \n",
    "# and/or the provided installation instructions are followed\n",
    "\n",
    "home = os.getenv(\"HOME\")\n",
    "\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "expander = matrix_expansion(active_motors)\n",
    "\n",
    "# Front back\n",
    "filename = home+\"/dep_matrices/front_back.dep\"\n",
    "fb_matrix = expander.load_from_file(filename)\n",
    "fb_reduced = expander.reduced_matrix(fb_matrix)\n",
    "#fb_expanded = expander.expanded_matrix(fb_reduced)\n",
    "\n",
    "# Front side\n",
    "filename = home+\"/dep_matrices/front_side.dep\"\n",
    "fs_matrix = expander.load_from_file(filename)\n",
    "fs_reduced = expander.reduced_matrix(fs_matrix)\n",
    "#fs_expanded = expander.expanded_matrix(fs_reduced)\n",
    "\n",
    "# Side down\n",
    "filename = home+\"/dep_matrices/side_down.dep\"\n",
    "sd_matrix = expander.load_from_file(filename)\n",
    "sd_reduced = expander.reduced_matrix(sd_matrix)\n",
    "#sd_expanded = expander.expanded_matrix(sd_reduced)\n",
    "\n",
    "# Zero\n",
    "zero_matrix = np.zeros(fb_matrix.shape)\n",
    "zero_reduced = np.zeros(fb_reduced.shape)\n",
    "\n",
    "#matrices = {\"fb\": fb_matrix, \"fs\": fs_matrix, \"sd\": sd_matrix, \"zero\": zero_matrix}\n",
    "matrices = {\"fb\": fb_reduced, \"fs\": fs_reduced, \"sd\": sd_reduced, \"zero\": zero_reduced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the ID and matrix pairs\n",
    "\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "n = len(active_motors)\n",
    "matrix_shape = (n,n*2)\n",
    "matrix_encoder = scalar_sdr(100,21,-0.25,0.25,matrix_shape,neg=False)\n",
    "shape = (brain_encoder.n,matrix_encoder.n*reduce(lambda x, y: x*y, matrix_encoder.ndarray_shape))\n",
    "\n",
    "matrix_lam = LAM(shape)\n",
    "for id_ in behaviors:    \n",
    "    brain_sig = brain_encoder.encode(behaviors[id_])\n",
    "    matrix = matrix_encoder.encode_ndarray(matrices[id_])\n",
    "    matrix_lam.store(brain_sig,matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_pickle = {\"brain_encoder\": brain_encoder, \"matrix_encoder\": matrix_encoder, \"lam\": matrix_lam, \"brain_id_to_behv_id\":dict(zip(brain_ids,names))}\n",
    "pickle_out = open(\"data/new/mem.pickle\",\"wb\") # note: another directory is used to not overwrite the previous files\n",
    "pickle.dump(mem_pickle, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section generates the weights for the trigger network. Notably, a seperate set of weights is used for each behavior and stored as a dictionary. This could just as well be implemented using an associative memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load recordings of base behaviors\n",
    "fb = pickle.load(open(\"data/bases/fb.pickle\",\"rb\"))\n",
    "fs = pickle.load(open(\"data/bases/fs.pickle\",\"rb\"))\n",
    "sd = pickle.load(open(\"data/bases/sd.pickle\", \"rb\"))\n",
    "zero = [np.zeros(fb[0].shape),np.zeros(fb[1].shape), np.zeros(fb[2].shape)]\n",
    "\n",
    "# define a dictionary for convenience\n",
    "bases = {\"fb\": fb, \"fs\": fs, \"sd\": sd, \"zero\": zero}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active motors\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "\n",
    "# define \"optimal\" transition points\n",
    "# note: manually obtained\n",
    "ts = [{},{}]\n",
    "ts[0] = {'fb': 124, 'fs': 126, 'sd': 118, 'zero': 0}\n",
    "ts[1] = {'fb': 62, 'fs': 63, 'sd': 59, 'zero': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = scalar_sdr(7,1,-100000.0,100000.0,shape=(6,1),neg=False)\n",
    "\n",
    "behvs = {}\n",
    "for id_ in bases:\n",
    "    behvs[id_] = {}\n",
    "    behvs[id_][\"weights_pos\"] = []\n",
    "    for i in active_motors:\n",
    "        m_pos = np.zeros((pos_encoder.n))\n",
    "        for j in range(2):\n",
    "            pos = pos_encoder.encode(float(bases[id_][0][ts[j][id_]][i]))\n",
    "            m_pos += pos # hebbian learning of weights with only direct connections\n",
    "        behvs[id_][\"weights_pos\"].append(m_pos)\n",
    "    behvs[id_][\"weights_pos\"] = np.array(behvs[id_][\"weights_pos\"]).reshape(1,6,pos_encoder.n,1)\n",
    "behvs[\"pos_encoder\"] = pos_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"data/new/neurons_pos.pickle\",\"wb\")  # note: another directory is used to not overwrite the previous files\n",
    "pickle.dump(behvs, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield behavior weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the memory node does not use Hopfield as default since the performance is degraded. However, this may be used, and the weights can be obtained as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for asynchronous Hopfield neural network\n",
    "# Note: memory capacity â‰ƒ 0.14*nodes\n",
    "class Hopfield_Neural_Network:\n",
    "    def __init__(self,nodes,iterations=100,weights=None):\n",
    "        self.nodes = nodes\n",
    "        self.iterations = iterations\n",
    "        try:\n",
    "            if weights == None:\n",
    "                self.weights = np.zeros((nodes,nodes))\n",
    "        except ValueError:\n",
    "            self.weights = weights\n",
    "    \n",
    "    def store(self,input):\n",
    "        dW = np.outer(input,input)\n",
    "        np.fill_diagonal(dW,0)\n",
    "        self.weights += dW\n",
    "        \n",
    "    def recall(self,input,range_=None):\n",
    "        # Can specify range of nodes to iterate over (i.e. nodes that are \"input\" may be known as correct)\n",
    "        if type(range_) == tuple:\n",
    "            a = range(range_[0],range_[1])\n",
    "        else:\n",
    "            a = self.nodes\n",
    "        update_sequence = np.random.choice(a, self.iterations)\n",
    "        for node in update_sequence:\n",
    "            input[node] = np.sign(np.inner(input,self.weights[:,node]))\n",
    "            if input[node] == 0: # this was missing\n",
    "                input[node] = -1\n",
    "        return input\n",
    "    \n",
    "    def setIter(self,iter_):\n",
    "        self.iterations = iter_\n",
    "    \n",
    "    def save_weights(self,filename):\n",
    "        np.save(filename, self.weights)\n",
    "        \n",
    "    def load_weights(self,filename):\n",
    "        weights = np.load(filename)\n",
    "        if weights.shape == (self.nodes, self.nodes):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions of specified weight array does not match network weight dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrix data for desired behaviors from matrix files generated by GUI\n",
    "# Note: need to define desired behaviors from set of files in /{HOME_ENVIRONMENT}/dep_matrices\n",
    "# The matrices are located in the \"home\" directory if new behaviors are generated with the GUI \n",
    "# and/or the provided installation instructions are followed\n",
    "\n",
    "home = os.getenv(\"HOME\")\n",
    "\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "expander = matrix_expansion(active_motors)\n",
    "\n",
    "# Front back\n",
    "filename = home+\"/dep_matrices/front_back.dep\"\n",
    "fb_matrix = expander.load_from_file(filename)\n",
    "fb_reduced = expander.reduced_matrix(fb_matrix)\n",
    "#fb_expanded = expander.expanded_matrix(fb_reduced)\n",
    "\n",
    "# Front side\n",
    "filename = home+\"/dep_matrices/front_side.dep\"\n",
    "fs_matrix = expander.load_from_file(filename)\n",
    "fs_reduced = expander.reduced_matrix(fs_matrix)\n",
    "#fs_expanded = expander.expanded_matrix(fs_reduced)\n",
    "\n",
    "# Side down\n",
    "filename = home+\"/dep_matrices/side_down.dep\"\n",
    "sd_matrix = expander.load_from_file(filename)\n",
    "sd_reduced = expander.reduced_matrix(sd_matrix)\n",
    "#sd_expanded = expander.expanded_matrix(sd_reduced)\n",
    "\n",
    "# Zero\n",
    "zero_matrix = np.zeros(fb_matrix.shape)\n",
    "zero_reduced = np.zeros(fb_reduced.shape)\n",
    "\n",
    "#matrices = {\"fb\": fb_matrix, \"fs\": fs_matrix, \"sd\": sd_matrix, \"zero\": zero_matrix}\n",
    "matrices = {\"fb\": fb_reduced, \"fs\": fs_reduced, \"sd\": sd_reduced, \"zero\": zero_reduced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_id = {\"zero\": 0.125, \"fb\": 0.375, \"fs\": 0.625, \"sd\": 0.875}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup encoders\n",
    "matrix_shape = (1,)\n",
    "m_encoder = scalar_sdr(80*20,44*20,-0.25,0.25,matrix_shape)\n",
    "b_encoder = scalar_sdr(92*20,40*20,0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of nodes of hopfield net\n",
    "nodes = m_encoder.nodes + b_encoder.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Initialize hopfield network for each matrix value\n",
    "# ii) Store data for each brain_id+matrix[brain_id] for given matrix index\n",
    "nns = {}\n",
    "for index in np.ndindex(matrices[\"zero\"].shape):\n",
    "    hnn = Hopfield_Neural_Network(nodes)\n",
    "    for id_ in brain_id:\n",
    "        data = np.array([])\n",
    "        matrix = matrices[id_][index].reshape(matrix_shape)\n",
    "        matrix = m_encoder.encode_ndarray(matrix)\n",
    "        data = np.append(data,matrix.flatten())\n",
    "\n",
    "        brain_sig = brain_id[id_]\n",
    "        brain_sig = b_encoder.encode(brain_sig)\n",
    "        data = np.append(data,brain_sig)\n",
    "\n",
    "        hnn.store(data)\n",
    "\n",
    "    nns[index] = hnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"data/new/hnns_80.pickle\",\"wb\")\n",
    "pickle.dump(nns, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
